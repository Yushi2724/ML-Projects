{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7GED80MHVXX3+9859kC6h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yushi2724/ML-Projects/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZnFqGtU7di_A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import re\n",
        "from nltk.stem import  WordNetLemmatizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from scipy.sparse import hstack\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from wordcloud import WordCloud\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Dataset.csv')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2XQAdQgudqJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "-sz8h10Mejoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_values = df['Review'].isnull().sum()\n",
        "print(\"Number of null values:\", null_values)\n",
        "\n",
        "\n",
        "different_null_values = df['Review'].apply(lambda x: x if pd.isnull(x) else None).unique()\n",
        "print(\"Different representations of null values:\", different_null_values)\n"
      ],
      "metadata": {
        "id": "N8er1k8Peyh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Rate'] = pd.to_numeric(df['Rate'], errors='coerce')\n",
        "\n",
        "# Filling null values in the 'Review' column based on the corresponding 'Rate' values\n",
        "df['Review'] = np.where((df['Review'].isnull()) & (df['Rate'] < 3), 'Not satisfied.', df['Review'])\n",
        "df['Review'] = np.where((df['Review'].isnull()) & (df['Rate'] == 3), 'Average quality', df['Review'])\n",
        "df['Review'] = np.where((df['Review'].isnull()) & (df['Rate'] > 3), 'Great product!', df['Review'])\n",
        "\n",
        "# Creating a new column 'Reviews' and assigning the values from the 'Review' column\n",
        "df['Reviews'] = df['Review']\n",
        "\n",
        "# Counting the number of remaining null values in the 'Reviews' column\n",
        "null_values = df['Reviews'].isnull().sum()\n",
        "print(\"Number of remaining null values:\", null_values)\n"
      ],
      "metadata": {
        "id": "7leljEUrezba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "columns= ['Review']\n",
        "df = df.drop(columns=columns)\n",
        "\n"
      ],
      "metadata": {
        "id": "8cRfU8YBfHOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Null Values Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q0v8SXBQfSNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()\n",
        "df =df.dropna()\n"
      ],
      "metadata": {
        "id": "nU-GIq5GfTWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns= ['product_name','product_price']\n",
        "df = df.drop(columns=columns)"
      ],
      "metadata": {
        "id": "Hk4CpJtefTpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Summary'] = df['Summary'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "df['Reviews'] = df['Reviews'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n"
      ],
      "metadata": {
        "id": "Oet0r9qFfhOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "df['Summary'] = df['Summary'].apply(word_tokenize)\n",
        "df['Reviews'] = df['Reviews'].apply(word_tokenize)\n"
      ],
      "metadata": {
        "id": "1inVWn-TflPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "# Removing stopwords from the 'Summary' and 'Reviews' columns\n",
        "df['Summary'] = df['Summary'].apply(lambda x: [word for word in x if word not in stopwords])\n",
        "df['Reviews'] = df['Reviews'].apply(lambda x: [word for word in x if word not in stopwords])\n",
        "# Joining the list of words back into a string for 'Summary' and 'Reviews' columns\n",
        "df['Summary'] = df['Summary'].apply(lambda x: ' '.join(x))\n",
        "df['Reviews'] = df['Reviews'].apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "bqvEoZEkfnPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "# Initializing the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# Lemmatizing the words in the 'Summary' and 'Reviews' columns\n",
        "df['Summary'] = df['Summary'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
        "df['Reviews'] = df['Reviews'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))"
      ],
      "metadata": {
        "id": "XsIzLDk8frCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "sentiment_mapping = {\"positive\": 2, \"negative\": 0,\"neutral\":1}\n",
        "df['Sentiment'] = df['Sentiment'].map(sentiment_mapping)"
      ],
      "metadata": {
        "id": "ceqz0RM8fxCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qk1NcnNEf53R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "positive_count = df['Sentiment'].value_counts()[2]\n",
        "negative_count = df['Sentiment'].value_counts()[0]\n",
        "neutral_count = df['Sentiment'].value_counts()[1]\n",
        "print(\"Number of positive sentiments:\", positive_count)\n",
        "print(\"Number of negative sentiments:\", negative_count)\n",
        "print(\"Number of neutral sentiments:\", neutral_count)\n",
        "pip install wordcloud\n",
        "# Concatenate the reviews into a single string\n",
        "all_reviews = ' '.join(df['Reviews'])\n",
        "# Generate a word cloud from the reviews\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_reviews)\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most Frequent Words in Reviews')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ct7fx2uvgEXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(df['Rate'], bins=5, edgecolor='k')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N06EBuVsgG0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = df['Sentiment'].value_counts()\n",
        "# Create a pie chart to visualize the sentiment distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HG7DJFeggKfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Sentiment', y='Rate', data=df)\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Rating')\n",
        "plt.title('Rating Distribution by Sentiment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bdRVBA_egQ8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 10\n",
        "top_words = df['Reviews'].str.split(expand=True).stack().value_counts().head(top_n)\n",
        "# Create a bar chart to visualize the top N words\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=top_words.values, y=top_words.index, palette='viridis')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Word')\n",
        "plt.title(f'Top {top_n} Words')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FDjzup12gUMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['Rate','Reviews','Summary']]\n",
        "y = df['Sentiment']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "WTDHcJIVgVTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "# Creating a pipeline for text feature extraction using TF-IDF\n",
        "text_pipeline = Pipeline([\n",
        "    ('tfidf', tfidf_vectorizer),\n",
        "])\n",
        "# Creating a numerical transformer to reshape the numerical features\n",
        "numerical_transformer = FunctionTransformer(lambda x: x.reshape(-1, 1), validate=False)\n",
        "# Creating a feature union to combine the text and numerical features\n",
        "feature_union = FeatureUnion([\n",
        "    ('text', text_pipeline),\n",
        "    ('numerical', numerical_transformer)\n",
        "])\n",
        "# Creating a pipeline to combine the features\n",
        "pipeline = Pipeline([\n",
        "    ('features', feature_union)\n",
        "])\n",
        "# Transforming the text features in the training set using TF-IDF\n",
        "X_train_text_transformed = text_pipeline.fit_transform(X_train['Reviews'])\n",
        "# Reshaping the numerical features in the training set\n",
        "X_train_numerical = X_train['Rate'].values.reshape(-1, 1)\n",
        "# Combining the text and numerical features in the training set\n",
        "X_train_final = hstack([X_train_text_transformed, X_train_numerical])\n",
        "# Transforming the text features in the test set using TF-IDF\n",
        "X_test_text_transformed = text_pipeline.transform(X_test['Reviews'])\n",
        "# Reshaping the numerical features in the test set\n",
        "X_test_numerical = X_test['Rate'].values.reshape(-1, 1)\n",
        "# Combining the text and numerical features in the test set\n",
        "X_test_final = hstack([X_test_text_transformed, X_test_numerical])\n"
      ],
      "metadata": {
        "id": "7_KT-FPCgYd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply SMOTE to handle the imbalanced classes\n",
        "oversampler = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_final, y_train)\n",
        "# Apply RandomUnderSampler to further balance the classes\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_resampled, y_train_resampled)\n",
        "# Count the number of samples in each sentiment class\n",
        "positive_count = sum(y_train_resampled == 2)\n",
        "negative_count = sum(y_train_resampled == 0)\n",
        "neutral_count = sum(y_train_resampled == 1)\n",
        "# Print the counts\n",
        "print(\"Number of positive samples:\", positive_count)\n",
        "print(\"Number of negative samples:\", negative_count)\n",
        "print(\"Number of neutral samples:\", neutral_count)"
      ],
      "metadata": {
        "id": "AFBWPl__gcPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Count the occurrences of each class in the resampled training data\n",
        "class_counts = y_train_resampled.value_counts()\n",
        "# Plot the class distribution\n",
        "plt.bar(class_counts.index, class_counts.values)\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Distribution after Sampling')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kFlY05-_ggrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "\n",
        "\n",
        "\n",
        "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
        "print(\"X_train_resampled data type:\", type(X_train_resampled))\n",
        "print(\"X_test_final shape:\", X_test_final.shape)\n",
        "print(\"X_test_final data type:\", type(X_test_final))\n"
      ],
      "metadata": {
        "id": "u8O1zllNgl5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing an XGBoost classifier\n",
        "xgb_classifier = xgb.XGBClassifier(random_state=42)\n",
        "# Fitting the XGBoost classifier on the resampled training data\n",
        "xgb_classifier.fit(X_train_resampled, y_train_resampled)\n",
        "# Predicting the sentiment labels for the test data\n",
        "y_pred = xgb_classifier.predict(X_test_final)\n",
        "# Printing the classification report to evaluate the model's performance\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "fcl4f5NLgqis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "# Fitting the Random Forest classifier on the resampled training data\n",
        "rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
        "# Predicting the sentiment labels for the test data\n",
        "y_pred = rf_classifier.predict(X_test_final)\n",
        "# Printing the classification report to evaluate the model's performance\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "3JXifIFCgup2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing a Logistic Regression model\n",
        "model = LogisticRegression()A\n",
        "# Fitting the Logistic Regression model on the resampled training data\n",
        "model.fit(X_train_resampled, y_train_resampled)\n",
        "# Predicting the sentiment labels for the test data\n",
        "y_pred = model.predict(X_test_final)\n",
        "# Printing the classification report to evaluate the model's performance\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "rWzhm8Aogy5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test_final)"
      ],
      "metadata": {
        "id": "0hNbfGlcg2hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "BWjQE9YGg6jc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}